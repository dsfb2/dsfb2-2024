{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BJmgP_HrGWUi"
      },
      "source": [
        "# NLP and Transformers\n",
        "\n",
        "In this tutorial the topics of natural language processing and Transformers will be covered. The aim of the tutorial is to introduce text classification techniques using classic Naive Bayes classifier method and tranformers. For Transformers, the pre-trained [BERT Base](https://huggingface.co/blog/bert-101) transformer from Google will be used.\n",
        "\n",
        "The tutorial is structured as follows:\n",
        "\n",
        "\n",
        "1.   How to make a computer understand words?\n",
        "2.   Naive Bayes classifier\n",
        "3.   Introduction to Transformers\n",
        "4.   Training BERT to suit our needs\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z4HDwiiVJTFF"
      },
      "source": [
        "# 1. How to make a computer understand words?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IvnFA78tKxBm"
      },
      "source": [
        "One of the question that arises when working with text data is\n",
        "> *How are we going to use the raw text data to train the model? The raw data is just a collection of strings!*\n",
        "\n",
        "From previous experience, we know that ML methods usually have some numerical input requirements. Moreover, these numerical feature vectors should have fixed size, what is hard to achieve working with text data. There raw text *document* has its own length.\n",
        "\n",
        "Therefore some techniques are requred to extract numerical features from the text content. To start with, there are two most basic ones: **tokenizing** and **counting**."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RV8CNwVXNID4"
      },
      "source": [
        " - **Tokenization** refers to the process of converting a sequence of text into smaller parts, known as tokens. These tokens can be as small as characters or as long as words.\n",
        " - **Counting** - calculation of tokens appearing in a *document*.\n",
        "\n",
        " In such setting each individual token occurrence frequency is treated as a **feature**. The vector of all the token frequencies for a given *document* is considered a multivariate sample.\n",
        " Below is the counting example."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xe4uzNctJW6q"
      },
      "outputs": [],
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "# example text\n",
        "corpus_example = [\n",
        "    'This is the first document.',\n",
        "    'This document is the second document.',\n",
        "    'And this is the third one.',\n",
        "    'Is this the first document?',\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g_sJwZz6OYeq"
      },
      "outputs": [],
      "source": [
        "# initialize the CountVectorizer() object\n",
        "vectorizer_example = CountVectorizer()\n",
        "\n",
        "# create the feature vector from the given corpus\n",
        "X_example = vectorizer_example.fit_transform(corpus_example)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ix_kThBMOf7U"
      },
      "outputs": [],
      "source": [
        "# print out the features array\n",
        "print(vectorizer_example.get_feature_names_out())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7MpI19qDWtoR"
      },
      "source": [
        "In the above toy example, we have a collection of strings stored into the variable corpus. Using the `CountVectorizer()`, we can see that we have a specific number of unique strings (vocabulary) in our data.\n",
        "This can be seen by printing the `vectorizer_example.get_feature_names_out()`. We observe that we have 9 unique words."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WM-d63iRPSbo"
      },
      "outputs": [],
      "source": [
        "# print out the frequency matrix\n",
        "print(X_example.toarray())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yRgEnta6XEF8"
      },
      "source": [
        "Next, we printed the transformed data `X_example.toarray()` and we observe the following:\n",
        "- We have 4 rows in `X_example` as the number of our text strings.\n",
        "- We have the same number of columns (features/variables) in the numerical representation of data (`X_example`) for all the samples. This was not the case before: the individual strings had different lengths.\n",
        "- The values 0,1,2, encode the frequency of a word that appeared in the initial text data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ODQffLBBP8XA"
      },
      "source": [
        "However, such an approach emphasizes a lot of the standard words, which do not bring a lot of meaning (such as *is*, *the*, *this*). Thus, it is beneficial to use the method which compensates this problem. This method is called **TF-IDF**, Term Frequency - Inverse Document Frequency."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wx29L9wOQ6I_"
      },
      "source": [
        "---\n",
        "## TF-IDF\n",
        "### Term Frequency (TF)\n",
        "\n",
        "Suppose we have a set of English text documents and wish to rank which document is most relevant to the query , \"Data Science is awesome !\" A simple way to start out is by eliminating documents that do not contain all four words \"Data\", \"Science\", \"is\", and \"awesome\", but this still leaves many documents. To further distinguish them, we might count the number of times each term occurs in each document; the number of times a term occurs in a document is called its **term frequency**. The weight of a term that occurs in a document is simply proportional to the term frequency.\n",
        "\n",
        "$$ tf(t, d)=\\frac{\\text{count of } t \\text{ in } d}{\\text{number of words in } d} $$\n",
        "\n",
        "### Document Frequncy (DF)\n",
        "\n",
        "This measures the importance of document in whole set of corpus, this is very similar to *TF*. The only difference is that *TF* is frequency counter for a term $t$ in document $d$, where as *DF* is the count of **occurrences** of term $t$ in the document set $N$. In other words, *DF* is the number of documents in which the word is present. We consider one occurrence if the term consists in the document at least once, we do not need to know the number of times the term is present.\n",
        "\n",
        "$$df(t) = \\text{occurrence of } t \\text{ in a set of documents } N$$\n",
        "\n",
        "### Inverse Document Frequency (IDF)\n",
        "\n",
        "While computing *TF*, all terms are considered equally important. However it is known that certain terms, such as \"is\", \"of\", and \"that\", may appear a lot of times but have little importance. Thus we need to weigh down the frequent terms while scale up the rare ones, by computing *IDF*, an *inverse document frequency* factor is incorporated which diminishes the weight of terms that occur very frequently in the document set and increases the weight of terms that occur rarely.\n",
        "*IDF* is the inverse of the document frequency which measures the informativeness of term $t$. When we calculate *IDF*, it will be very low for the most occurring words. This finally gives what we want, a relative weight.\n",
        "\n",
        "$$idf(t) = \\frac{N}{df(t)}$$\n",
        "\n",
        "There are few problems with the *IDF*. In case of a large corpus, say $100\\,000 \\, 000$, the *IDF* value explodes. To avoid the effect we take the log of *idf*.\n",
        "During the query time, when a word which is not in vocab occurs, the *df* will be 0. As we cannot divide by 0, we smoothen the value by adding 1 to the denominator. Then the final formula will be:\n",
        "\n",
        "$$idf(t) = \\log\\frac{N}{df(t)+1}$$\n",
        "\n",
        "This gives us the **tf-idf** formula:\n",
        "\n",
        "$$tf\\text{-}idf(t,d) = tf(t,d) +  \\log\\frac{N}{df(t)+1}$$\n",
        "\n",
        "In Python it will look the following way:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xMcqZpHYVRYT"
      },
      "outputs": [],
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "# initialize the TfidfVectorizer() object\n",
        "tf_idf_example = TfidfVectorizer()\n",
        "\n",
        "# create the feature vector from the given corpus\n",
        "X_tf_idf = tf_idf_example.fit_transform(corpus_example)\n",
        "\n",
        "# print out the features array\n",
        "print(tf_idf_example.get_feature_names_out())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ELWRqmYvXlTK"
      },
      "source": [
        "Here we received the same 9 unique words from the example above."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7pUd59VIWCCr"
      },
      "outputs": [],
      "source": [
        "# print out the weight matrix\n",
        "print(X_tf_idf.toarray())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zKEdzKE8Xs_3"
      },
      "source": [
        "Now, instead fo frequencies we get the tf-idf weights."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mnKXRlpGWKTH"
      },
      "source": [
        "This is the way we can transform text into numerical representation. Now let's have a look into text classification."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SnrVzr3zJX9r"
      },
      "source": [
        "# 2. Naive Bayes classifier"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-75hOmpQWX8I"
      },
      "source": [
        "Naive Bayes classifiers are a collection of classification algorithms based on **Bayes' Theorem**. It is not a single algorithm but a family of algorithms where all of them share a common principle, i.e. every pair of features being classified is independent of each other.\n",
        "\n",
        "The text dataset we would like to classify is divided into two parts, namely, **feature matrix** and the **response/target vector**.\n",
        "- The **feature matrix ($X$)** contains all the vectors (rows) of the dataset in which each vector consists of the value of dependent features. The number of features is $d$ i.e. $X = (x_1,x_2,x_3,...,x_d)$, where $x_i$ is a numerical representation of a word. All vectors share the same length $d$.\n",
        "- The **response/target vector** ($y$) contains the value of the class/group variable for each row of feature matrix."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1zrNO_UgZBuj"
      },
      "source": [
        "Naive Bayes assumes that each feature/variable of the same class makes an independent and equal contribution to the outcome. These assumption are not generally correct in real-world situations. In-fact, the independence assumption is often not met and this is why it is called \"Naive\" as it assumes something that might not be true."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4zRc-vlHZVNm"
      },
      "source": [
        "Given a data matrix $X$ and a target vector $y$ and following the Bayes' Theorem we state our problem as:\n",
        "$$P(y|X)=\\frac{P(y)*(P(X|y)}{P(X)}$$\n",
        "Here $P(y|X)$ is the probability of observing the class $y$ given the sample $X$."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_UQykPgLZ8IW"
      },
      "source": [
        "Now the *naive* conditional independence assumptions come into play. Let's assume that all features in $X$ are mutually independent, conditional on the category $y$:\n",
        "$$P(y|X)=\\frac{P(y)∏_{i=1}^dP(x_i|y)}{P(x_1)P(x_2)...P(x_d)}$$\n",
        "\n",
        "The denominator remains constant for a given input, so we can remove it:\n",
        "$$P(y|X) \\propto P(y)∏_{i=1}^dP(x_i|y)$$\n",
        "\n",
        "Finally, to find the probability of a given sample for all possible values of the class variable $y$, we just need to find the output with maximum probability:\n",
        "$$y= \\arg\\max_y P(y)∏_{i=1}^dP(x_i|y)$$\n",
        "\n",
        "Let's see how we can do this in Python:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "oskMI9y1WXi3"
      },
      "outputs": [],
      "source": [
        "# import dependencies\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "%matplotlib inline\n",
        "%config InlineBackend.figure_format='retina' # high-resolution plots\n",
        "\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.metrics import confusion_matrix, accuracy_score, classification_report\n",
        "\n",
        "# url to our dataset\n",
        "url = 'https://raw.githubusercontent.com/dsfb2/dsfb2-2024/main/assignment_3/data/bbc-text.csv'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XMmMyKtEdNHy"
      },
      "source": [
        "In this tutorial we will use the [BBC News](https://www.kaggle.com/c/learn-ai-bbc) dataset from Kaggle Competition. The datset is comprised of 2225 articles, each labeled under one of 5 categories: business, entertainment, politics, sport or tech."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v1nf6UfvJaOQ"
      },
      "outputs": [],
      "source": [
        "# load data\n",
        "df = pd.read_csv(url)\n",
        "\n",
        "print(f'[LOG] Dataset contains: {len(df)} entries')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NaUN72qWfTr0"
      },
      "outputs": [],
      "source": [
        "# explore data\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KnrpB3dy_83y"
      },
      "outputs": [],
      "source": [
        "# explore the text\n",
        "print(f'Text: {df.text[0]}')\n",
        "print(f'Text length: {len(df.text[0])}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "myUioA8qAJvg"
      },
      "source": [
        "We can see that the text is clean and uncased, so we can use it without any preparation to create a classifier. Pieces of news are very long, therefore we can assume a great performance even on Bayes Classifier."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wOjwQ9KCkW9e"
      },
      "outputs": [],
      "source": [
        "# check the unique categories\n",
        "print(f'[LOG] Unique news categories: {df[\"category\"].unique()}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hJ8O9v1tCLDp"
      },
      "outputs": [],
      "source": [
        "# explore the distribution of categories\n",
        "plt.bar(df[\"category\"].value_counts().index, df[\"category\"].value_counts())\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QZp_t8Ni_P77"
      },
      "source": [
        "As our categories are strings, we will convert them into numerical classes. It is possible to apply one-hot encoding."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wkJDmg017bzw"
      },
      "outputs": [],
      "source": [
        "# create the encoding for classes\n",
        "mapping = {}\n",
        "map_value = 0\n",
        "for cat in df[\"category\"].unique():\n",
        "  mapping[cat] = map_value\n",
        "  map_value+=1\n",
        "\n",
        "print(mapping)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "573j-p_sTEOU"
      },
      "outputs": [],
      "source": [
        "# encode news classes creating a new column\n",
        "df[\"label\"] = df[\"category\"].map(mapping)\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "06qb8DcV_tHo"
      },
      "source": [
        "We will now split our dataset into training and test parts to run the Bayes' Classifier"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZVGoyAkiTW2n"
      },
      "outputs": [],
      "source": [
        "# create feature and label vectors\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(df['text'], df['label'], test_size=0.3, random_state=42)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I2Atf6eZAtpx"
      },
      "source": [
        "Here the pipeline is built to simplify the process. Simply saying, this is the order in which data will be transformed. It is possible to build these pipelines in scikit. At first, the TF-IDF process is applied. Then Multinomial Naive Bayes classifier is used to detect classes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qnZMWFpcTJlE"
      },
      "outputs": [],
      "source": [
        "# build, train and predict using the Naive Bayes model\n",
        "model_NB = make_pipeline(TfidfVectorizer(), MultinomialNB())\n",
        "model_NB.fit(X_train, y_train)\n",
        "y_hat = model_NB.predict(X_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W3K92I66Thht"
      },
      "outputs": [],
      "source": [
        "# let's look into the model perforance:\n",
        "print(f'Overall accuracy: {accuracy_score(y_test, y_hat):.3f}')\n",
        "print('Classification report:')\n",
        "print(classification_report(y_test, y_hat))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dlNgGQMtm2Or"
      },
      "source": [
        "We can see that Naive Bayes classifier performs amazingly well. It is a rather rare case, when a baseline model has such a high accuracy. In our case this is attributed to the high quality of data and the length of indivudal pieces of text."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yz8nRGf7C4Yf"
      },
      "outputs": [],
      "source": [
        "con_mat = confusion_matrix(y_test, y_hat)\n",
        "sns.heatmap(con_mat.T, square=True, annot=True, fmt='d', cbar=False)\n",
        "plt.xlabel('True labels')\n",
        "plt.ylabel('Predicted labels')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YFSwzwTKJa56"
      },
      "source": [
        "# 3. Introduction to Transformers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qhrDTuYGE4Sw"
      },
      "source": [
        "## Who (what) is BERT?\n",
        "Introduced by Google in 2018, [BERT](https://blog.research.google/2018/11/open-sourcing-bert-state-of-art-pre.html) is an acronym for Bidirectional Encoder Representations from Transformers. The name itself gives us several clues to what BERT is all about.\n",
        "BERT architecture consists of several Transformer encoders stacked together. Each Transformer encoder encapsulates two sub-layers: a self-attention layer and a feed-forward layer.\n",
        "\n",
        "There are two different BERT models:\n",
        "- BERT base, which is a BERT model consists of 12 layers of Transformer encoder, 12 attention heads, **768 hidden size**, and 110M parameters - **we will use this own to speed up computations**\n",
        "- BERT large, which is a BERT model consists of 24 layers of Transformer encoder,16 attention heads, 1024 hidden size, and 340 parameters.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mYLzKKVlFoZB"
      },
      "source": [
        "There are at least two reasons why BERT is a powerful language model:\n",
        "It is pre-trained on unlabeled data extracted from BooksCorpus, which has 800M words, and from Wikipedia, which has 2,500M words.\n",
        "As the name suggests, it is pre-trained by utilizing the bidirectional nature of the encoder stacks. This means that BERT learns information from a sequence of words not only from left to right, but also from right to left."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hSXGdjS2Fwqx"
      },
      "source": [
        "We are not going (thankfully) to train the transformer ourselves. It took Google 4 days and a huuuuuuuuuuge amount of computational power (4 TPUs) to train BERT, not to say about all preliminary attempts. Instead we will take a model checkpont, load it and build a classifier on top of it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OChctr5CJePR"
      },
      "outputs": [],
      "source": [
        "# install the library that contains checkpoints of models and tokenizers\n",
        "!pip install transformers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8NIyOG6yHvD9"
      },
      "source": [
        "BERT model expects a sequence of tokens (words) as an input. In each sequence of tokens, there are two special tokens that BERT would expect as an input:\n",
        "- [CLS]: This is the first token of every sequence, which stands for classification token.\n",
        "- [SEP]: This is the token that makes BERT know which token belongs to which sequence. This special token is mainly important for a next sentence prediction task or question-answering task. If we only have one sequence, then this token will be appended to the end of the sequence.\n",
        "\n",
        "It is also important to note that the maximum size of tokens that can be fed into BERT model is 512. If the tokens in a sequence are less than 512, we can use padding to fill the unused token slots with [PAD] token. If the tokens in a sequence are longer than 512, then we need to do a truncation.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "liBPf3VANFH8"
      },
      "outputs": [],
      "source": [
        "# import the tokenizer, which will create tokens in the correct way\n",
        "from transformers import BertTokenizerFast\n",
        "\n",
        "# we will use an uncased version, because our data contains only lower-case text\n",
        "tokenizer = BertTokenizerFast.from_pretrained('bert-base-uncased', lower=True) # for languages other than english you can use bert-base-multilingual-cased or language-specific versions of bert\n",
        "\n",
        "example_text = 'i will try to code a good transformer tonight'\n",
        "\n",
        "\n",
        "bert_input = tokenizer(\n",
        "    example_text,           # text we want to tokenize\n",
        "    max_length = 15,        # maximum length of text sequence we accept (limited to 512)\n",
        "    padding='max_length',   # how we should behave if text length < max length (to pad each sequence to the maximum length that you specify.)\n",
        "    truncation=True,        # how we should behave if text length > max length (if True, then the tokens in each sequence that exceed the maximum length will be truncated.)\n",
        "    return_tensors=\"pt\")    # return Pytorch type of tensors, tf for TensorFlow\n",
        "\n",
        "\n",
        "print(bert_input['input_ids'])  # the id representation of each token\n",
        "print(bert_input['token_type_ids']) # a binary mask that identifies in which sequence a token belongs. If we only have a single sequence, then all of the token type ids will be 0. For a text classification task, token_type_ids is an optional input for our BERT model.\n",
        "print(bert_input['attention_mask']) # a binary mask that identifies whether a token is a real word or just padding. If the token contains [CLS], [SEP], or any real word, then the mask would be 1. Meanwhile, if the token is just padding or [PAD], then the mask would be 0.\n",
        "\n",
        "example_text = tokenizer.decode(bert_input.input_ids[0])\n",
        "\n",
        "print(example_text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OAl0au7aIBH-"
      },
      "source": [
        "BERT model then will output an embedding vector of size 768 in each of the tokens. We can use these vectors as an input for different kinds of NLP applications, whether it is text classification, next sentence prediction, Named-Entity-Recognition (NER), or question-answering.\n",
        "\n",
        "For a text classification task, we focus our attention on the embedding vector output from the special [CLS] token. This means that we're going to use the embedding vector of size 768 from [CLS] token as an input for our classifier, which then will output a vector of size the number of classes in our classification task."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IWIwkVjCM7Je"
      },
      "source": [
        "Before going further, let's specify some parameters that we will use further"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0V0HigTyMN-h"
      },
      "outputs": [],
      "source": [
        "# Defining some key variables that will be used later on in the training\n",
        "MAX_LEN = 512             # max length of sequence. we will use all 512 as our text articles are long.\n",
        "TRAIN_BATCH_SIZE = 4      # how many sequences are included in the training batch\n",
        "VALID_BATCH_SIZE = 4      # how many sequences are included in the validation batch\n",
        "EPOCHS = 5                # how many epochs we will use during the training process\n",
        "LEARNING_RATE = 1e-05     # our learning rate\n",
        "TOKENIZER = BertTokenizerFast.from_pretrained('bert-base-uncased', lower=True) # our tokenizer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q_GeteiuNewe"
      },
      "source": [
        "Now that we know what kind of output that we will get from `BertTokenizerFast` , let's build a `NewsDataset` class for our news dataset that will serve as a class to generate our news data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cRC5I21sNxHz"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "class NewsDataset(torch.utils.data.Dataset):\n",
        "\n",
        "    def __init__(self, df):\n",
        "\n",
        "        self.labels = [label for label in df['label']] # create labels for each article\n",
        "        self.texts = [TOKENIZER(text,\n",
        "                                padding='max_length',\n",
        "                                max_length = MAX_LEN,\n",
        "                                truncation=True,\n",
        "                                return_tensors=\"pt\") for text in df['text']] # create tokens for each article\n",
        "\n",
        "    def classes(self):\n",
        "        return self.labels\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "\n",
        "    def get_batch_labels(self, idx):\n",
        "        # Fetch a batch of labels\n",
        "        return np.array(self.labels[idx])\n",
        "\n",
        "    def get_batch_texts(self, idx):\n",
        "        # Fetch a batch of inputs\n",
        "        return self.texts[idx]\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "\n",
        "        batch_texts = self.get_batch_texts(idx)\n",
        "        batch_y = self.get_batch_labels(idx)\n",
        "\n",
        "        return batch_texts, batch_y"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oykndemKOmNY"
      },
      "source": [
        "After defining dataset class, let's split our dataframe into training and validation sets with the proportion of 80:20."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bizsbF4BOtYz"
      },
      "outputs": [],
      "source": [
        "seed = 42\n",
        "np.random.seed(seed)\n",
        "df_train, df_val = np.split(df.sample(frac=1, random_state=seed), [int(.8*len(df))])\n",
        "\n",
        "print(f'Training set length is {len(df_train)} and validation set length {len(df_val)}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vm0VyLLXOTwN"
      },
      "source": [
        "# 4. Training BERT to suit our needs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ROukNRLqPxKg"
      },
      "source": [
        "BERT model outputs two variables:\n",
        "- The first variable, which we name `_`, contains the embedding vectors of all of the tokens in a sequence.\n",
        "- The second variable, which we name `pooled_output`, contains the embedding vector of [CLS] token. For a text classification task, it is enough to use this embedding as an input for our classifier.\n",
        "\n",
        "We then pass the `pooled_output` variable into a linear layer with ReLU activation function. At the end of the linear layer, we have a vector of size 5, each corresponds to a category of our labels (tech, business, sport, entertainment politics)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5ZdxZke7Z4-9"
      },
      "outputs": [],
      "source": [
        "from torch import nn\n",
        "from transformers import BertModel\n",
        "\n",
        "class BertClassifier(nn.Module):\n",
        "\n",
        "    def __init__(self, dropout=0.3):\n",
        "\n",
        "        super(BertClassifier, self).__init__()\n",
        "\n",
        "        self.bert = BertModel.from_pretrained('bert-base-uncased') # pre-trained transformer\n",
        "        self.dropout = nn.Dropout(dropout)                         # pool with dropout\n",
        "        self.linear = nn.Linear(768, 5)                            # classification fully-connected layer\n",
        "        self.relu = nn.ReLU()                                      # ReLU activation function\n",
        "\n",
        "    def forward(self, input_id, mask):\n",
        "\n",
        "        _, pooled_output = self.bert(input_ids= input_id, attention_mask=mask,return_dict=False)\n",
        "        dropout_output = self.dropout(pooled_output)\n",
        "        linear_output = self.linear(dropout_output)\n",
        "        final_layer = self.relu(linear_output)\n",
        "\n",
        "        return final_layer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import sys\n",
        "import os\n",
        "import getpass\n",
        "\n",
        "# before we define our training loop, we create the folder, where we will save our model checkpoints\n",
        "# check if running on Google Colab\n",
        "if 'google.colab' in str(get_ipython()):\n",
        "    \n",
        "    print('Running on Colab')\n",
        "    \n",
        "    # import the Google Colab GDrive connector\n",
        "    from google.colab import drive\n",
        "\n",
        "    # mount GDrive inside the Colab notebook\n",
        "    drive.mount('/content/drive')\n",
        "    \n",
        "    # name Colab Notebooks directory\n",
        "    CHECKPOINT_DIRECTORY = '/content/drive/MyDrive/Colab Notebooks/dsfb2/a3_tutorial'\n",
        "    \n",
        "else:\n",
        "    # check if running on MacOS\n",
        "    if sys.platform == 'darwin':\n",
        "        print('Running on MacOS')\n",
        "\n",
        "        # get the username\n",
        "        user_name = getpass.getuser()\n",
        "\n",
        "        # name main directory\n",
        "        CHECKPOINT_DIRECTORY = f\"/Users/{user_name}/dsfb2/a3_tutorial\"\n",
        "\n",
        "    # check if running on Windows\n",
        "    elif sys.platform == 'win32':\n",
        "        print('Running on Windows')\n",
        "\n",
        "        # get the username\n",
        "        user_name = getpass.getuser()\n",
        "\n",
        "        # name main directory\n",
        "        CHECKPOINT_DIRECTORY = f\"C:/Users/{user_name}/dsfb2/a3_tutorial\"\n",
        "\n",
        "# create the main directory\n",
        "if not os.path.exists(CHECKPOINT_DIRECTORY): os.makedirs(CHECKPOINT_DIRECTORY)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pQjEDJNEQPF4"
      },
      "outputs": [],
      "source": [
        "from torch.optim import Adam\n",
        "from tqdm import tqdm\n",
        "import os\n",
        "\n",
        "# function for training and validation\n",
        "def train_validate(model, train_data, val_data, learning_rate, epochs):\n",
        "\n",
        "    # create tokenized datasets for training and validation\n",
        "    train, val = NewsDataset(train_data), NewsDataset(val_data)\n",
        "\n",
        "    # create loaders for tensors\n",
        "    train_dataloader = torch.utils.data.DataLoader(train, batch_size=TRAIN_BATCH_SIZE, shuffle=True)\n",
        "    val_dataloader = torch.utils.data.DataLoader(val, batch_size=VALID_BATCH_SIZE)\n",
        "\n",
        "    # activate GPU computing\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'mps' if torch.backends.mps.is_available() else 'cpu').type\n",
        "    print('[LOG] notebook with {} computation enabled'.format(str(device)))\n",
        "\n",
        "    # initialize loss function\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    # initialize optimizer\n",
        "    optimizer = Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "    # send model and loss function to computational device\n",
        "    model = model.to(device)\n",
        "    criterion = criterion.to(device)\n",
        "\n",
        "    # initialize empty lists for storing data\n",
        "    history_val_loss=[]     # average training loss for epoch\n",
        "    history_train_loss = [] # average validation loss for epoch\n",
        "    history_val_acc=[]      # training accuracy for epoch\n",
        "    history_train_acc = []  # validation accuracy for epoch\n",
        "\n",
        "    # training and validation cycle\n",
        "    for epoch in range(epochs):\n",
        "\n",
        "        # set the model to the training mode (gradients are updated)\n",
        "        model.train()\n",
        "\n",
        "        # initialize list for storing loss for each propagation\n",
        "        loss_train = []\n",
        "\n",
        "        # initilize lists for storing actual and predicted labels\n",
        "        train_label_list = []\n",
        "        train_output_list = []\n",
        "\n",
        "################## TRAINING ##################\n",
        "\n",
        "        # get our train input and label tensors for loader, tdqm is just a nice progress bar\n",
        "        for train_input, train_label in tqdm(train_dataloader):\n",
        "\n",
        "            # send training label, attention mask and id to device\n",
        "            train_label = train_label.to(device)\n",
        "            mask = train_input['attention_mask'].to(device)\n",
        "            input_id = train_input['input_ids'].squeeze(1).to(device)\n",
        "\n",
        "            # receive predicted label\n",
        "            output = model(input_id, mask)\n",
        "\n",
        "            # calculate the loss value between actual and predicted label\n",
        "            batch_loss = criterion(output, train_label.long())\n",
        "\n",
        "            # store the loss\n",
        "            loss_train.append(batch_loss.item())\n",
        "\n",
        "            # save actual and predicted values\n",
        "            train_label_list.extend(train_label.cpu().detach().numpy().tolist())\n",
        "            train_output_list.extend(torch.sigmoid(output).cpu().detach().numpy().tolist())\n",
        "\n",
        "            # reset graph gradients\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # run backward pass to update the weights\n",
        "            batch_loss.backward()\n",
        "\n",
        "            # update network paramaters\n",
        "            optimizer.step()\n",
        "\n",
        "        # calculate average training loss\n",
        "        total_loss_train = np.mean(loss_train)\n",
        "        # append average training loss\n",
        "        history_train_loss.append(total_loss_train)\n",
        "        # calculate training accuracy\n",
        "        acc_train = accuracy_score(np.array(train_label_list).astype(int), np.argmax(np.array(train_output_list), axis=1))\n",
        "        # append training accuracy\n",
        "        history_train_acc.append(acc_train)\n",
        "\n",
        "################## VALIDATION ##################\n",
        "\n",
        "        # initialize list for storing loss for each propagation\n",
        "        loss_val = []\n",
        "\n",
        "        # initilize lists for storing actual and predicted labels\n",
        "        val_label_list = []\n",
        "        val_output_list = []\n",
        "\n",
        "        # set the model to the validation mode (gradients are not updated)\n",
        "        model.eval()\n",
        "        with torch.no_grad():\n",
        "\n",
        "            for val_input, val_label in val_dataloader:\n",
        "\n",
        "                # send validation label, attention mask and id to device\n",
        "                val_label = val_label.to(device)\n",
        "                mask = val_input['attention_mask'].to(device)\n",
        "                input_id = val_input['input_ids'].squeeze(1).to(device)\n",
        "\n",
        "                # receive predicted label\n",
        "                output = model(input_id, mask)\n",
        "\n",
        "                # calculate the loss value between actual and predicted label\n",
        "                batch_loss = criterion(output, val_label.long())\n",
        "\n",
        "                # store the loss\n",
        "                loss_val.append(batch_loss.item())\n",
        "\n",
        "                # save actual and predicted values\n",
        "                val_label_list.extend(val_label.cpu().detach().numpy().tolist())\n",
        "                val_output_list.extend(torch.sigmoid(output).cpu().detach().numpy().tolist())\n",
        "\n",
        "        # calculate average validation loss\n",
        "        total_loss_val = np.mean(loss_val)\n",
        "        # append average validation loss\n",
        "        history_val_loss.append(total_loss_val)\n",
        "        # calculate validation accuracy\n",
        "        acc_val = accuracy_score(np.array(val_label_list).astype(int), np.argmax(np.array(val_output_list), axis=1))\n",
        "        # append validation accuracy\n",
        "        history_val_acc.append(acc_val)\n",
        "\n",
        "        print(f'Epochs: {epoch} | Train Loss: {total_loss_train: .3f} | Train Accuracy: {acc_train: .3f} | Val Loss: {total_loss_val: .3f} | Val Accuracy: {acc_val: .3f}')\n",
        "        model_name = f'{epoch}_news_classifier.pth'\n",
        "        model_path = os.path.join(CHECKPOINT_DIRECTORY, model_name)\n",
        "        torch.save(model.state_dict(), model_path)\n",
        "\n",
        "    return history_train_loss, history_val_loss, history_train_acc, history_val_acc\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KjcKp29GT7HE"
      },
      "outputs": [],
      "source": [
        "# initialize our model\n",
        "model = BertClassifier()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bew0_F0bUQIk"
      },
      "source": [
        "Make sure that training cycle begins and move to Azure labs. Expected runtime for one epoch is 3 minutes on Azure labs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# check that gpu is activated\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'mps' if torch.backends.mps.is_available() else 'cpu').type\n",
        "\n",
        "if device == 'cuda':\n",
        "    !nvidia-smi\n",
        "elif device == 'mps':\n",
        "    print('Using Apple M-series SoC GPU accelerator')\n",
        "else:\n",
        "    print('Using CPU')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yk4irJ7qUBZI"
      },
      "outputs": [],
      "source": [
        "# train the model\n",
        "train_loss, val_loss, train_acc, val_acc = train_validate(model, df_train, df_val, LEARNING_RATE, EPOCHS)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oTCMHg2xVnTn"
      },
      "source": [
        "# 5. Validation and graphical representation of results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rQAK2IuLX2EC"
      },
      "source": [
        "We can plot our loss and accuracy to examine the training cycle"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E6FhlzGyUNKs"
      },
      "outputs": [],
      "source": [
        "# plotting the loss\n",
        "plt.plot(np.array(range(1, len(train_loss)+1)), train_loss, c='b', label='training error')\n",
        "plt.plot(np.array(range(1, len(val_loss)+1)), val_loss, c='r', label='validation error')\n",
        "plt.xlabel(\"[training epoch $e_i$]\", fontsize=10)\n",
        "plt.ylabel(\"[Classification Error]\", fontsize=10)\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YZDiboAXYMlc"
      },
      "outputs": [],
      "source": [
        "# plotting the accuracy\n",
        "plt.plot(np.array(range(1, len(train_acc)+1)), train_acc, c='b', label='training accuracy')\n",
        "plt.plot(np.array(range(1, len(val_acc)+1)), val_acc, c='r', label='validation accuracy')\n",
        "plt.xlabel(\"[training epoch $e_i$]\", fontsize=10)\n",
        "plt.ylabel(\"[Accuracy]\", fontsize=10)\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zqDQPee2Y-lJ"
      },
      "source": [
        "We will use a separate validation function to load a saved state and make a confusion matrix"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9OmoQU2bY-Ld"
      },
      "outputs": [],
      "source": [
        "def evaluate(model, test_data):\n",
        "\n",
        "    # create tokenized dataset\n",
        "    test = NewsDataset(test_data)\n",
        "\n",
        "    # create loaders for tensors\n",
        "    val_dataloader = torch.utils.data.DataLoader(test, batch_size=VALID_BATCH_SIZE)\n",
        "\n",
        "    # activate GPU computing\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'mps' if torch.backends.mps.is_available() else 'cpu').type\n",
        "    print('[LOG] notebook with {} computation enabled'.format(str(device)))\n",
        "\n",
        "    # initialize loss function\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    model = model.to(device)\n",
        "    criterion = criterion.to(device)\n",
        "\n",
        "    loss_val = []\n",
        "\n",
        "    val_label_list = []\n",
        "    val_output_list = []\n",
        "\n",
        "    # set the model to the validation mode (gradients are not updated)\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "\n",
        "        for val_input, val_label in val_dataloader:\n",
        "\n",
        "            # send validation label, attention mask and id to device\n",
        "            val_label = val_label.to(device)\n",
        "            mask = val_input['attention_mask'].to(device)\n",
        "            input_id = val_input['input_ids'].squeeze(1).to(device)\n",
        "\n",
        "            # receive predicted label\n",
        "            output = model(input_id, mask)\n",
        "\n",
        "            # calculate the loss value between actual and predicted label\n",
        "            batch_loss = criterion(output, val_label.long())\n",
        "            loss_val.append(batch_loss.item())\n",
        "\n",
        "            # store the loss\n",
        "            val_label_list.extend(val_label.cpu().detach().numpy().tolist())\n",
        "            val_output_list.extend(torch.sigmoid(output).cpu().detach().numpy().tolist())\n",
        "\n",
        "        # save actual and predicted values\n",
        "        total_loss_val = np.mean(loss_val)\n",
        "        acc_val = accuracy_score(np.array(val_label_list).astype(int), np.argmax(np.array(val_output_list), axis=1))\n",
        "\n",
        "    print(f'Test Accuracy: {acc_val: .3f}')\n",
        "\n",
        "    # return actual and predicted values\n",
        "    return np.array(val_label_list).astype(int), np.argmax(np.array(val_output_list), axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_BAhqSfZY6Ik"
      },
      "outputs": [],
      "source": [
        "# load saved state\n",
        "epoch_num = 1\n",
        "model_name = f'{epoch_num}_news_classifier.pth'\n",
        "model_path = os.path.join(CHECKPOINT_DIRECTORY, model_name)\n",
        "\n",
        "model = BertClassifier()\n",
        "model.load_state_dict(torch.load(model_path, map_location=torch.device('cpu')))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ri2Xn4ymaO8c"
      },
      "outputs": [],
      "source": [
        "# evaluate model\n",
        "label, prediction = evaluate(model, df_val)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6YRX8ukzac9j"
      },
      "outputs": [],
      "source": [
        "# create classification report\n",
        "print(classification_report(label, prediction))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pAMhxQRGakcy"
      },
      "outputs": [],
      "source": [
        "# create confusion matrix\n",
        "con_mat = confusion_matrix(label, prediction)\n",
        "sns.heatmap(con_mat.T, square=True, annot=True, fmt='d', cbar=False)\n",
        "plt.xlabel('True labels')\n",
        "plt.ylabel('Predicted labels')\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "cqrtraffic",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
